{"cells":[{"cell_type":"markdown","metadata":{"id":"kFD7DVZ-xKdT"},"source":["# Assignment 3: Tiền xử lý văn bản với tiếng Việt\n","Tổng quan: ở bài tập này chúng ta xây dựng chương trình tiền xử lý văn bản với tiếng Việt."]},{"cell_type":"markdown","metadata":{"id":"XOAeiqdrxKdt"},"source":["Import các thư viện cần dùng. Lưu ý ở đây ta, ta dùng thư viện underthesea để dùng tokenize tiếng Việt, để cài đặt các bạn làm theo hướng dẫn sau ([link](https://github.com/undertheseanlp/underthesea))"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"RrFQ_Ht_xKdu"},"outputs":[],"source":["import os,glob\n","import codecs\n","import sys\n","import re\n","from underthesea import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"hC27lBQZxKdw"},"source":["## Câu hỏi 1: tạo corpus và khảo sát dữ liệu\n","Dữ liệu trong phần này được trích một phần từ bộ dữ liệu [VNTC](https://github.com/duyvuleo/VNTC). VNTC là bộ dữ liệu về tin tức tiếng Việt với nhiều chủ đề khác nhau. Trong nội dung phần này ta chỉ xử lý cho chủ đề khoa học trong VNTC. Ta sẽ tạo một corpus từ cả thư mục train và test. Hoàn thiện đoạn chương trình:\n","- ghi sentences_list ra 1 file với tên dataset_name.txt, mỗi phần tử là một doc, sẽ trên 1 dòng.\n","- Kiểm tra xem trong corpus có bao nhiêu docs"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"GyNKT8wAxKdx","outputId":"b2eb7c10-da8d-49cb-8b7d-4f6543700cbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["train labels = test labels\n","Number of documents:  3916\n"]}],"source":["dataset_name = \"VNTC_khoahoc\"\n","path = ['./VNTC_khoahoc/Train_Full/', './VNTC_khoahoc/Test_Full/']\n","\n","if os.listdir(path[0]) == os.listdir(path[1]): \n","    folder_list = [os.listdir(path[0]), os.listdir(path[1])]\n","    print(\"train labels = test labels\")\n","else:\n","    print(\"train labels differ from test labels\")\n","\n","doc_num = 0\n","sentences_list = []\n","meta_data_list = []\n","for i in range(2): \n","    for folder_name in folder_list[i]:\n","        folder_path = path[i] + folder_name\n","        if folder_name[0] != \".\":\n","            for file_name in glob.glob(os.path.join(folder_path, '*.txt')):\n","                f = codecs.open(file_name, 'br')\n","                file_content = (f.read().decode(\"utf-16\")).replace(\"\\r\\n\", \" \")\n","                sentences_list.append(file_content.strip()) \n","                f.close\n","                doc_num += 1\n","\n","print(\"Number of documents: \", doc_num)"]},{"cell_type":"markdown","metadata":{"id":"3KXHcDpuxKd0"},"source":["## Câu hỏi 2: Viết các hàm tiền xử lý\n","### Câu hỏi 2.1: Viết hàm làm sạch văn bản.\n","Gợi ý:\n","- Biết rằng văn bản chỉ giữ lại các ký tự sau: aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\'\\`\n","- Sau đó cắt các khoảng trắng của văn bản đầu vào."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World! This is a test  1234567890 àÀảẢãÃáÁạẠăĂ\n"]}],"source":["import re\n","\n","def clean_str(text):\n","    \"\"\"\n","    Cleans the input text by keeping only the specified characters and trimming whitespace.\n","    \n","    Parameters:\n","    text (str): The input text to be cleaned.\n","    \n","    Returns:\n","    str: The cleaned text.\n","    \"\"\"\n","    # Define the regex pattern to keep only the specified characters\n","    pattern = r\"[^aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\'\\`]\"\n","    \n","    # Replace unwanted characters with a space\n","    cleaned_text = re.sub(pattern, ' ', text)\n","    \n","    # Trim whitespace\n","    cleaned_text = cleaned_text.strip()\n","    \n","    return cleaned_text\n","\n","# Example usage\n","sample_text = \"Hello World! This is a test. 1234567890 àÀảẢãÃáÁạẠăĂ\"\n","cleaned_sample = clean_str(sample_text)\n","print(cleaned_sample)"]},{"cell_type":"markdown","metadata":{"id":"9KfXstqAxKd1"},"source":["## Câu hỏi 2.2: Viết hàm chuyển văn bản về dạng thường"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KRwgVjxhxKd1"},"outputs":[],"source":["# make all text lowercase\n","def text_lowercase(string):\n","    return string.lower()"]},{"cell_type":"markdown","metadata":{"id":"rYM_GO_5xKd2"},"source":["## Câu hỏi 2.3: tách từ\n","Gợi ý: sử dụng hàm word_tokenize() đã import ở trên với hai tham số lần lượt là strings, và format=\"text\"."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pty34NwyxKd2"},"outputs":[],"source":["def tokenize(strings):\n","    return word_tokenize(strings, format=\"text\")"]},{"cell_type":"markdown","metadata":{"id":"-gQGmL4gxKd2"},"source":["## Câu hỏi 2.4 Loại bỏ từ dừng\n","Ở đây để loại bỏ từ dừng, ta dùng một danh sách các từ dừng tiếng Việt. Chúng được lưu trong file './vietnamese-stopwords.txt'. Hoàn thiện chương trình sau:\n","- Đối chiếu từng từ trong văn bản (strings) nếu từ nào không có trong từ dừng thì thêm vào doc_words"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"aqStv2rPxKd3"},"outputs":[],"source":["def remove_stopwords(strings):\n","    strings = strings.split()\n","    f = open('./vietnamese-stopwords.txt', 'r', encoding='utf-8')  \n","    stopwords = f.readlines()\n","    stop_words = [s.replace(\"\\n\", '') for s in stopwords]\n","    doc_words = []\n","    for word in strings:\n","        if word not in stop_words:\n","            doc_words.append(word)\n","    doc_str = ' '.join(doc_words).strip()\n","\n","    return doc_str"]},{"cell_type":"markdown","metadata":{"id":"jUNOKigIxKd4"},"source":["## Câu hỏi 2.5 xây dựng hàm tiền xử lý\n","Gợi ý: gọi lần lượt các hàm clean_str, text_lowercase, tokenize, remove_stopwords, rồi trả ra kết quả cho hàm."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"_vd-el91xKd_"},"outputs":[],"source":["def text_preprocessing(strings):\n","    strings = clean_str(strings)\n","    strings = text_lowercase(strings)\n","    strings = tokenize(strings)\n","    strings = remove_stopwords(strings)\n","    return strings"]},{"cell_type":"markdown","metadata":{"id":"1BGOqa1mxKeA"},"source":["## Câu hỏi 3: thực hiện tiền xử lý\n","Công việc giờ chúng ta sẽ đọc corpus từ file đã làm ở câu hỏi 1. Sau đó ta sẽ gọi hàm tiền xử lý cho từng doc trong corpus trên.\n","\n","Gợi ý: gọi hàm text_preprocessing() với tham số đầu vào là doc_content, lưu vào biến temp1"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","length of clean_docs =  3916\n","clean_docs[0]:\n","ninh thuận_địa_điểm ưu_tiên nhà_máy điện hạt_nhân góc ninh_thuận , địa_điểm ưu_tiên lựa_chọn nhà_máy điện hạt_nhân vương_hữu , viện trưởng viện năng_lượng nguyên_tử việt_nam , địa_điểm hiện cân_nhắc lựa_chọn nhà_máy điện hạt_nhân ninh_thuận , phú_yên phan_rang , ninh_thuận địa_điểm ưu_tiên lựa_chọn tiến_triển dự_án xây_dựng nhà_máy điện hạt_nhân vn , vương_hữu , viện năng_lượng nguyên_tử việt_nam hoàn_tất dự_án tiền_khả_thi trình chính_phủ phê_duyệt dự_kiến , ( 2007 ) , dự_án khả_thi đáp_ứng kịp tiến_độ xây_dựng nhà_máy điện hạt_nhân vn kế_hoạch , nhà_máy điện hạt_nhân xây_dựng đi hoạt_động 2020 quy_mô công_suất 2 000 mw 4 000 mw , chiếm 5 9 tổng_công_suất phát_điện toàn_quốc lựa_chọn công_nghệ nhà_máy điện hạt_nhân vn , viện năng_lượng nguyên_tử việt_nam , hiện phân_tích trưng_cầu ý_kiến chuyên_gia , công_nghệ nhà_máy điện hạt_nhân chọn_lựa cơ_sở an_toàn vận_hành hướng_dẫn cơ_quan năng_lượng nguyên_tử quốc_tế ( iaea ) , nhà_máy điện hạt_nhân hoạt_động 3 500 4 500 , 500 700 trình_độ đại_học đại_học , 700 1 000 kỹ_thuật_viên 2 200 3 000 công_nhân lành_nghề , viện 681 cán_bộ trung_bình 42 , đại_học 361 , thạc_sỹ 78 , tiến_sỹ gs , pgs 62 tuổi_đời trung_bình cán_bộ viện năng_lượng nguyên_tử việt_nam 42 độ vn trẻ , quốc_tế già ! đại_diện khoa_học công_nghệ nhận_xét\n"]}],"source":["doc_content_list = []\n","\n","for doc in sentences_list:\n","    doc_content_list.append(text_preprocessing(doc))\n","\n","clean_docs = []\n","for doc_content in doc_content_list:\n","    temp1 = doc_content\n","    clean_docs.append(temp1)\n","print(\"\\nlength of clean_docs = \", len(clean_docs))\n","print('clean_docs[0]:\\n' + clean_docs[0])"]},{"cell_type":"markdown","metadata":{"id":"SFhai6BwxKeB"},"source":["## Câu 4: ghi dữ liệu đã tiền xử lý\n","Gợi ý: ghi clean_docs vào trong file có tên dataset_name + '.clean.txt', trong đó mỗi doc (phần tử trong clean_docs) ghi trên một dòng."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"xfHmSiRrxKeB"},"outputs":[],"source":["clean_corpus_str = '\\n'.join(clean_docs)\n","f = open(dataset_name + '.clean.txt', 'w', encoding='utf-8')\n","f.write(clean_corpus_str)\n","f.close()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Text_preprocessing_vietnamese.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
