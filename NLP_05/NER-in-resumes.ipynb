{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"NER-in-resumes.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"oVpDppKjnSLB","outputId":"72def9fa-bfbf-4d2b-c214-dd061896db74"},"source":["import json\n","import random\n","import logging\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_recall_fscore_support\n","from spacy.gold import GoldParse\n","from spacy.scorer import Scorer\n","from sklearn.metrics import accuracy_score\n","import spacy"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'spacy'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-92fcec21d30b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoldParse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mScorer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"]}]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"jify5wuanSLJ"},"source":["def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n","    try:\n","        training_data = []\n","        lines=[]\n","        with open(dataturks_JSON_FilePath, 'r', encoding=\"utf8\") as f:\n","            lines = # YOUR CODE HERE\n","\n","        for line in lines:\n","            data = json.loads(line)\n","            text = data['content']\n","            entities = []\n","            for annotation in data['annotation']:\n","                #only a single point in text annotation.\n","                point = None # YOUR CODE HERE\n","                labels = None # YOUR CODE HERE\n","                # handle both list of labels or a single label.\n","                if not isinstance(labels, list):\n","                    labels = [labels]\n","\n","                for label in labels:\n","                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n","                    entities.append((point['start'], point['end'] + 1 ,label))\n","\n","\n","            training_data.append((text, {\"entities\" : entities}))\n","\n","        return training_data\n","    except Exception as e:\n","        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n","        return None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"_asqlghHnSLK"},"source":["TRAIN_DATA = convert_dataturks_to_spacy(\"traindata.json\")\n","nlp = None # YOUR CODE HERE  # create blank Language class\n","# create the built-in pipeline components and add them to the pipeline\n","# nlp.create_pipe works for built-ins that are registered with spaCy\n","if 'ner' not in nlp.pipe_names:\n","    ner = nlp.create_pipe('ner')\n","    nlp.add_pipe(ner, last=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"fOn2p33HnSLK"},"source":["# add labels\n","for _, annotations in TRAIN_DATA:\n","     for ent in annotations.get('entities'):\n","        # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z19Xr8CRnSLK","outputId":"d53b28d7-02e4-4964-8ef8-efb5fe362be9"},"source":["# get names of other pipes to disable them during training\n","other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n","with nlp.disable_pipes(*other_pipes):  # only train NER\n","    optimizer = nlp.begin_training()\n","    for itn in range(10):\n","        print(\"Statring iteration \" + str(itn))\n","        random.shuffle(TRAIN_DATA)\n","        losses = {}\n","        for text, annotations in TRAIN_DATA:\n","            nlp.update(\n","                None,  # batch of texts\n","                None,  # batch of annotations\n","                None,  # dropout - make it harder to memorise data\n","                None,  # callable to update weights\n","                losses=losses)\n","        print(losses)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n","Statring iteration 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"1g41W4TOnSLL"},"source":["#test the model and evaluate it\n","examples = convert_dataturks_to_spacy(\"testdata.json\")\n","tp=0\n","tr=0\n","tf=0\n","\n","ta=0\n","c=0        \n","for text,annot in examples:\n","\n","    f=open(\"resume\"+str(c)+\".txt\",\"w\")\n","    doc_to_test=nlp(text)\n","    d={}\n","    for ent in doc_to_test.ents:\n","        d[ent.label_]=[]\n","    for ent in doc_to_test.ents:\n","        d[ent.label_].append(ent.text)\n","\n","    for i in set(d.keys()):\n","\n","        f.write(\"\\n\\n\")\n","        f.write(i +\":\"+\"\\n\")\n","        for j in set(d[i]):\n","            f.write(j.replace('\\n','')+\"\\n\")\n","    d={}\n","    for ent in doc_to_test.ents:\n","        d[ent.label_]=[0,0,0,0,0,0]\n","    for ent in doc_to_test.ents:\n","        doc_gold_text= nlp.make_doc(text)\n","        gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n","        y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]\n","        y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  \n","        if(d[ent.label_][0]==0):\n","            #f.write(\"For Entity \"+ent.label_+\"\\n\")   \n","            #f.write(classification_report(y_true, y_pred)+\"\\n\")\n","            (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')\n","            a=accuracy_score(y_true,y_pred)\n","            d[ent.label_][0]=1\n","            d[ent.label_][1]+=p\n","            d[ent.label_][2]+=r\n","            d[ent.label_][3]+=f\n","            d[ent.label_][4]+=a\n","            d[ent.label_][5]+=1\n","    c+=1\n","for i in d:\n","    print(\"\\n For Entity \"+i+\"\\n\")\n","    print(\"Accuracy : \"+str((d[i][4]/d[i][5])*100)+\"%\")\n","    print(\"Precision : \"+str(d[i][1]/d[i][5]))\n","    print(\"Recall : \"+str(d[i][2]/d[i][5]))\n","    print(\"F-score : \"+str(d[i][3]/d[i][5]))"],"execution_count":null,"outputs":[]}]}