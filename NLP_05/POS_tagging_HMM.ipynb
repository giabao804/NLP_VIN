{"cells":[{"cell_type":"markdown","metadata":{"id":"rKjEToVwjcHS"},"source":["# Gán nhãn từ loại với mô hình Markov ẩn\n","Notebook này hướng dẫn sử dụng mô hình Markov ẩn (HMM) để thực hiện gán nhãn dữ liệu. Chúng ta sẽ sử dụng Python để xây dựng một mô hình gán nhãn dữ liệu bằng HMM và thuật toán Veterbi"]},{"cell_type":"markdown","metadata":{"id":"1GVIpXirjcHd"},"source":["Đọc dữ liệu và tổ chức dữ liệu huấn luyện"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"9sppY7_0jcHe","outputId":"e8b65b63-a918-4865-f62b-83c81c6a4f7b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package treebank to /home/giabao/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to\n","[nltk_data]     /home/giabao/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n"]}],"source":["# Importing libraries\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import random\n","from sklearn.model_selection import train_test_split\n","import pprint, time\n"," \n","#download the treebank corpus from nltk\n","nltk.download('treebank')\n"," \n","#download the universal tagset from nltk\n","nltk.download('universal_tagset')\n"," \n","# reading the Treebank tagged sentences\n","nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n"," \n","#print the first two sentences along with tags\n","print(nltk_data[:2])"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"reNG91RbjcHg"},"outputs":[{"name":"stdout","output_type":"stream","text":["('Pierre', 'NOUN')\n","('Vinken', 'NOUN')\n","(',', '.')\n","('61', 'NUM')\n","('years', 'NOUN')\n","('old', 'ADJ')\n","(',', '.')\n","('will', 'VERB')\n","('join', 'VERB')\n","('the', 'DET')\n","('board', 'NOUN')\n","('as', 'ADP')\n","('a', 'DET')\n","('nonexecutive', 'ADJ')\n","('director', 'NOUN')\n","('Nov.', 'NOUN')\n","('29', 'NUM')\n","('.', '.')\n","('Mr.', 'NOUN')\n","('Vinken', 'NOUN')\n","('is', 'VERB')\n","('chairman', 'NOUN')\n","('of', 'ADP')\n","('Elsevier', 'NOUN')\n","('N.V.', 'NOUN')\n","(',', '.')\n","('the', 'DET')\n","('Dutch', 'NOUN')\n","('publishing', 'VERB')\n","('group', 'NOUN')\n","('.', '.')\n"]}],"source":["#print each word with its respective tag for first two sentences\n","for sent in nltk_data[:2]:\n","  for tuple in sent:\n","    print(tuple)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"TCxhNM0cjcHg"},"outputs":[],"source":["# split data into training and validation set in the ratio 80:20\n","# YOUR CODE HERE\n","\n","train_set, test_set = train_test_split(nltk_data, test_size=0.2, random_state=42)   \n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1vu7UnLujcHh"},"outputs":[{"name":"stdout","output_type":"stream","text":["80127\n","20549\n"]}],"source":["# create list of train and test tagged words\n","# YOUR CODE HERE\n","train_tagged_words = [tup for sent in train_set for tup in sent]    \n","test_tagged_words = [tup for sent in test_set for tup in sent]\n","\n","print(len(train_tagged_words))\n","print(len(test_tagged_words))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"H3KChYG5jcHh"},"outputs":[{"data":{"text/plain":["[('Pierre', 'NOUN'),\n"," ('Vinken', 'NOUN'),\n"," (',', '.'),\n"," ('61', 'NUM'),\n"," ('years', 'NOUN')]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# check some of the tagged words.\n","train_tagged_words[:5]"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"NWCrU1Y4jcHi"},"outputs":[{"name":"stdout","output_type":"stream","text":["12\n","{'CONJ', 'ADP', 'X', 'ADV', 'VERB', 'NUM', '.', 'DET', 'ADJ', 'PRT', 'NOUN', 'PRON'}\n"]}],"source":["#use set datatype to check how many unique tags are present in training data\n","tags = {tag for word,tag in train_tagged_words}\n","print(len(tags))\n","print(tags)\n"," \n","# check total words in vocabulary\n","vocab = {word for word,tag in train_tagged_words}"]},{"cell_type":"markdown","metadata":{"id":"ziCwbpxojcHi"},"source":["Tính Emission Probability và Transition Probability từ dữ liệu huấn luyện"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"beFG6Cy-jcHi"},"outputs":[],"source":["# compute Emission Probability\n","def word_given_tag(word, tag, train_bag = train_tagged_words):\n","    # YOUR CODE HERE\n","#now calculate the total number of times the passed word occurred as the passed tag.\n","    # YOUR CODE HERE\n","    tag_list = [pair for pair in train_bag if pair[1]==tag]\n","    count_tag = len(tag_list)\n","    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n","    count_w_given_tag = len(w_given_tag_list)\n","\n"," \n","    return (count_w_given_tag, count_tag)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"obAOpcq9jcHj"},"outputs":[],"source":["# compute  Transition Probability\n","def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n","    tags = [pair[1] for pair in train_bag]\n","    count_t1 = len([t for t in tags if t==t1])\n","    count_t2_t1 = 0\n","    for index in range(len(tags)-1):\n","        if tags[index]==t1 and tags[index+1] == t2:\n","            count_t2_t1 += 1\n","    return (count_t2_t1, count_t1)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"LssUgzFzjcHj"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[5.57413616e-04 5.35117052e-02 7.80379027e-03 5.68561889e-02\n","  1.51059091e-01 3.95763665e-02 2.95429211e-02 1.21516168e-01\n","  1.20958753e-01 3.90189514e-03 3.53400230e-01 6.13154955e-02]\n"," [1.02184189e-03 1.64771993e-02 3.24434787e-02 1.37948655e-02\n","  8.68565589e-03 6.30987361e-02 3.92131805e-02 3.27628046e-01\n","  1.03972413e-01 1.27730239e-03 3.23412955e-01 6.89743236e-02]\n"," [1.10413097e-02 1.44869596e-01 7.55758584e-02 2.68418044e-02\n","  2.02741295e-01 3.04587861e-03 1.61431566e-01 5.40643446e-02\n","  1.52293928e-02 1.86179325e-01 6.18694089e-02 5.71102239e-02]\n"," [7.48916063e-03 1.22585729e-01 2.44383123e-02 8.43515992e-02\n","  3.38983059e-01 3.11391409e-02 1.35199055e-01 6.54316097e-02\n","  1.30863219e-01 1.34016555e-02 3.07449736e-02 1.53724868e-02]\n"," [5.08130062e-03 9.13710296e-02 2.19235033e-01 8.11160356e-02\n","  1.71008870e-01 2.34663710e-02 3.47376205e-02 1.32390976e-01\n","  6.47634864e-02 3.02106421e-02 1.09848484e-01 3.67701389e-02]\n"," [1.38346935e-02 3.54735740e-02 2.01489896e-01 3.54735716e-03\n","  1.70273148e-02 1.84107840e-01 1.19191200e-01 3.19262152e-03\n","  3.15714777e-02 2.94430647e-02 3.59347284e-01 1.77367858e-03]\n"," [5.72588407e-02 8.99476558e-02 2.74543315e-02 5.24516627e-02\n","  9.05886143e-02 7.99059942e-02 9.51821357e-02 1.71669692e-01\n","  4.43328694e-02 2.45700241e-03 2.22625792e-01 6.60185888e-02]\n"," [4.32900444e-04 9.23520885e-03 4.50216457e-02 1.18326116e-02\n","  3.86724398e-02 2.26551220e-02 1.83261186e-02 5.33910515e-03\n","  2.07359314e-01 2.88600277e-04 6.37229443e-01 3.60750360e-03]\n"," [1.79098602e-02 7.77406022e-02 2.10588463e-02 4.72347951e-03\n","  1.27927577e-02 1.83034837e-02 6.53414652e-02 5.11710299e-03\n","  6.69159591e-02 9.84058250e-03 6.99468613e-01 7.87246623e-04]\n"," [2.34283484e-03 2.22569313e-02 1.36665367e-02 1.05427569e-02\n","  4.04919952e-01 5.93518168e-02 4.21710275e-02 1.00351423e-01\n","  8.86372477e-02 1.56188989e-03 2.37407267e-01 1.67903155e-02]\n"," [4.22264859e-02 1.75972775e-01 2.91833878e-02 1.67510025e-02\n","  1.47312865e-01 9.72779654e-03 2.41842613e-01 1.33484555e-02\n","  1.16908047e-02 4.42767404e-02 2.62737751e-01 4.92933160e-03]\n"," [4.51671192e-03 2.30352301e-02 9.25925896e-02 3.43270116e-02\n","  4.81481493e-01 5.87172527e-03 4.33604345e-02 9.93676577e-03\n","  7.27190599e-02 1.26467934e-02 2.10930437e-01 8.58175289e-03]]\n"]}],"source":["# creating t x t transition matrix of tags, t= no of tags\n","# Matrix(i, j) represents P(jth tag after the ith tag)\n"," \n","tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n","for i, t1 in enumerate(list(tags)):\n","    for j, t2 in enumerate(list(tags)): \n","        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n"," \n","print(tags_matrix)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ASg10dMpjcHk"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CONJ</th>\n","      <th>ADP</th>\n","      <th>X</th>\n","      <th>ADV</th>\n","      <th>VERB</th>\n","      <th>NUM</th>\n","      <th>.</th>\n","      <th>DET</th>\n","      <th>ADJ</th>\n","      <th>PRT</th>\n","      <th>NOUN</th>\n","      <th>PRON</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>CONJ</th>\n","      <td>0.000557</td>\n","      <td>0.053512</td>\n","      <td>0.007804</td>\n","      <td>0.056856</td>\n","      <td>0.151059</td>\n","      <td>0.039576</td>\n","      <td>0.029543</td>\n","      <td>0.121516</td>\n","      <td>0.120959</td>\n","      <td>0.003902</td>\n","      <td>0.353400</td>\n","      <td>0.061315</td>\n","    </tr>\n","    <tr>\n","      <th>ADP</th>\n","      <td>0.001022</td>\n","      <td>0.016477</td>\n","      <td>0.032443</td>\n","      <td>0.013795</td>\n","      <td>0.008686</td>\n","      <td>0.063099</td>\n","      <td>0.039213</td>\n","      <td>0.327628</td>\n","      <td>0.103972</td>\n","      <td>0.001277</td>\n","      <td>0.323413</td>\n","      <td>0.068974</td>\n","    </tr>\n","    <tr>\n","      <th>X</th>\n","      <td>0.011041</td>\n","      <td>0.144870</td>\n","      <td>0.075576</td>\n","      <td>0.026842</td>\n","      <td>0.202741</td>\n","      <td>0.003046</td>\n","      <td>0.161432</td>\n","      <td>0.054064</td>\n","      <td>0.015229</td>\n","      <td>0.186179</td>\n","      <td>0.061869</td>\n","      <td>0.057110</td>\n","    </tr>\n","    <tr>\n","      <th>ADV</th>\n","      <td>0.007489</td>\n","      <td>0.122586</td>\n","      <td>0.024438</td>\n","      <td>0.084352</td>\n","      <td>0.338983</td>\n","      <td>0.031139</td>\n","      <td>0.135199</td>\n","      <td>0.065432</td>\n","      <td>0.130863</td>\n","      <td>0.013402</td>\n","      <td>0.030745</td>\n","      <td>0.015372</td>\n","    </tr>\n","    <tr>\n","      <th>VERB</th>\n","      <td>0.005081</td>\n","      <td>0.091371</td>\n","      <td>0.219235</td>\n","      <td>0.081116</td>\n","      <td>0.171009</td>\n","      <td>0.023466</td>\n","      <td>0.034738</td>\n","      <td>0.132391</td>\n","      <td>0.064763</td>\n","      <td>0.030211</td>\n","      <td>0.109848</td>\n","      <td>0.036770</td>\n","    </tr>\n","    <tr>\n","      <th>NUM</th>\n","      <td>0.013835</td>\n","      <td>0.035474</td>\n","      <td>0.201490</td>\n","      <td>0.003547</td>\n","      <td>0.017027</td>\n","      <td>0.184108</td>\n","      <td>0.119191</td>\n","      <td>0.003193</td>\n","      <td>0.031571</td>\n","      <td>0.029443</td>\n","      <td>0.359347</td>\n","      <td>0.001774</td>\n","    </tr>\n","    <tr>\n","      <th>.</th>\n","      <td>0.057259</td>\n","      <td>0.089948</td>\n","      <td>0.027454</td>\n","      <td>0.052452</td>\n","      <td>0.090589</td>\n","      <td>0.079906</td>\n","      <td>0.095182</td>\n","      <td>0.171670</td>\n","      <td>0.044333</td>\n","      <td>0.002457</td>\n","      <td>0.222626</td>\n","      <td>0.066019</td>\n","    </tr>\n","    <tr>\n","      <th>DET</th>\n","      <td>0.000433</td>\n","      <td>0.009235</td>\n","      <td>0.045022</td>\n","      <td>0.011833</td>\n","      <td>0.038672</td>\n","      <td>0.022655</td>\n","      <td>0.018326</td>\n","      <td>0.005339</td>\n","      <td>0.207359</td>\n","      <td>0.000289</td>\n","      <td>0.637229</td>\n","      <td>0.003608</td>\n","    </tr>\n","    <tr>\n","      <th>ADJ</th>\n","      <td>0.017910</td>\n","      <td>0.077741</td>\n","      <td>0.021059</td>\n","      <td>0.004723</td>\n","      <td>0.012793</td>\n","      <td>0.018303</td>\n","      <td>0.065341</td>\n","      <td>0.005117</td>\n","      <td>0.066916</td>\n","      <td>0.009841</td>\n","      <td>0.699469</td>\n","      <td>0.000787</td>\n","    </tr>\n","    <tr>\n","      <th>PRT</th>\n","      <td>0.002343</td>\n","      <td>0.022257</td>\n","      <td>0.013667</td>\n","      <td>0.010543</td>\n","      <td>0.404920</td>\n","      <td>0.059352</td>\n","      <td>0.042171</td>\n","      <td>0.100351</td>\n","      <td>0.088637</td>\n","      <td>0.001562</td>\n","      <td>0.237407</td>\n","      <td>0.016790</td>\n","    </tr>\n","    <tr>\n","      <th>NOUN</th>\n","      <td>0.042226</td>\n","      <td>0.175973</td>\n","      <td>0.029183</td>\n","      <td>0.016751</td>\n","      <td>0.147313</td>\n","      <td>0.009728</td>\n","      <td>0.241843</td>\n","      <td>0.013348</td>\n","      <td>0.011691</td>\n","      <td>0.044277</td>\n","      <td>0.262738</td>\n","      <td>0.004929</td>\n","    </tr>\n","    <tr>\n","      <th>PRON</th>\n","      <td>0.004517</td>\n","      <td>0.023035</td>\n","      <td>0.092593</td>\n","      <td>0.034327</td>\n","      <td>0.481481</td>\n","      <td>0.005872</td>\n","      <td>0.043360</td>\n","      <td>0.009937</td>\n","      <td>0.072719</td>\n","      <td>0.012647</td>\n","      <td>0.210930</td>\n","      <td>0.008582</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          CONJ       ADP         X       ADV      VERB       NUM         .  \\\n","CONJ  0.000557  0.053512  0.007804  0.056856  0.151059  0.039576  0.029543   \n","ADP   0.001022  0.016477  0.032443  0.013795  0.008686  0.063099  0.039213   \n","X     0.011041  0.144870  0.075576  0.026842  0.202741  0.003046  0.161432   \n","ADV   0.007489  0.122586  0.024438  0.084352  0.338983  0.031139  0.135199   \n","VERB  0.005081  0.091371  0.219235  0.081116  0.171009  0.023466  0.034738   \n","NUM   0.013835  0.035474  0.201490  0.003547  0.017027  0.184108  0.119191   \n",".     0.057259  0.089948  0.027454  0.052452  0.090589  0.079906  0.095182   \n","DET   0.000433  0.009235  0.045022  0.011833  0.038672  0.022655  0.018326   \n","ADJ   0.017910  0.077741  0.021059  0.004723  0.012793  0.018303  0.065341   \n","PRT   0.002343  0.022257  0.013667  0.010543  0.404920  0.059352  0.042171   \n","NOUN  0.042226  0.175973  0.029183  0.016751  0.147313  0.009728  0.241843   \n","PRON  0.004517  0.023035  0.092593  0.034327  0.481481  0.005872  0.043360   \n","\n","           DET       ADJ       PRT      NOUN      PRON  \n","CONJ  0.121516  0.120959  0.003902  0.353400  0.061315  \n","ADP   0.327628  0.103972  0.001277  0.323413  0.068974  \n","X     0.054064  0.015229  0.186179  0.061869  0.057110  \n","ADV   0.065432  0.130863  0.013402  0.030745  0.015372  \n","VERB  0.132391  0.064763  0.030211  0.109848  0.036770  \n","NUM   0.003193  0.031571  0.029443  0.359347  0.001774  \n",".     0.171670  0.044333  0.002457  0.222626  0.066019  \n","DET   0.005339  0.207359  0.000289  0.637229  0.003608  \n","ADJ   0.005117  0.066916  0.009841  0.699469  0.000787  \n","PRT   0.100351  0.088637  0.001562  0.237407  0.016790  \n","NOUN  0.013348  0.011691  0.044277  0.262738  0.004929  \n","PRON  0.009937  0.072719  0.012647  0.210930  0.008582  "]},"metadata":{},"output_type":"display_data"}],"source":["# convert the matrix to a df for better readability\n","#the table is same as the transition table shown in section 3 of article\n","tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n","display(tags_df)"]},{"cell_type":"markdown","metadata":{"id":"CyTI5y3gjcHk"},"source":["Xây dựng thuật toán Viterbi"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"hwYFFe0gjcHk"},"outputs":[],"source":["def Viterbi(words, train_bag = train_tagged_words):\n","    state = []\n","    T = list(set([pair[1] for pair in train_bag]))\n","     \n","    for key, word in enumerate(words):\n","        #initialise list of probability column for a given observation\n","        p = [] \n","        for tag in T:\n","            if key == 0:\n","                transition_p = tags_df.loc['.', tag]\n","            else:\n","                transition_p = tags_df.loc[state[-1], tag]\n","                 \n","            # compute emission and state probabilities\n","            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n","            state_probability = emission_p * transition_p    \n","            p.append(state_probability)\n","             \n","        pmax = max(p)\n","        # getting state for which probability is maximum\n","        state_max = T[p.index(pmax)] \n","        state.append(state_max)\n","    return list(zip(words, state))"]},{"cell_type":"markdown","metadata":{"id":"AFpk340zjcHl"},"source":["Test thuật toán Viterbi"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"VreK4UbDjcHl"},"outputs":[],"source":["# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n","random.seed(1234)      #define a random seed to get same sentences when run multiple times\n"," \n","# choose random 10 numbers\n","rndom = [random.randint(1,len(test_set)) for x in range(10)]\n"," \n","# list of 10 sents on which we test the model\n","test_run = [test_set[i] for i in rndom]\n"," \n","# list of tagged words\n","test_run_base = [tup for sent in test_run for tup in sent]\n"," \n","# list of untagged words\n","test_tagged_words = [tup[0] for sent in test_run for tup in sent]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"yOwmeCi5jcHm"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken in seconds:  25.746501207351685\n","Viterbi Algorithm Accuracy:  88.96321070234113\n"]}],"source":["#Here We will only test 10 sentences to check the accuracy\n","#as testing the whole training set takes huge amount of time\n","start = time.time()\n","tagged_seq = Viterbi(test_tagged_words)\n","end = time.time()\n","difference = end-start\n"," \n","print(\"Time taken in seconds: \", difference)\n"," \n","# accuracy\n","check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n"," \n","accuracy = len(check)/len(tagged_seq)\n","print('Viterbi Algorithm Accuracy: ',accuracy*100)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"THcSxf_5jcHn"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m test_untagged_words \u001b[38;5;241m=\u001b[39m [tup[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m test_set \u001b[38;5;28;01mfor\u001b[39;00m tup \u001b[38;5;129;01min\u001b[39;00m sent]\n\u001b[1;32m      8\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m tagged_seq \u001b[38;5;241m=\u001b[39m \u001b[43mViterbi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_untagged_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m difference \u001b[38;5;241m=\u001b[39m end\u001b[38;5;241m-\u001b[39mstart\n","Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mViterbi\u001b[0;34m(words, train_bag)\u001b[0m\n\u001b[1;32m     12\u001b[0m     transition_p \u001b[38;5;241m=\u001b[39m tags_df\u001b[38;5;241m.\u001b[39mloc[state[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], tag]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# compute emission and state probabilities\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m emission_p \u001b[38;5;241m=\u001b[39m \u001b[43mword_given_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mword_given_tag(words[key], tag)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m state_probability \u001b[38;5;241m=\u001b[39m emission_p \u001b[38;5;241m*\u001b[39m transition_p    \n\u001b[1;32m     17\u001b[0m p\u001b[38;5;241m.\u001b[39mappend(state_probability)\n","Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mword_given_tag\u001b[0;34m(word, tag, train_bag)\u001b[0m\n\u001b[1;32m      6\u001b[0m tag_list \u001b[38;5;241m=\u001b[39m [pair \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m train_bag \u001b[38;5;28;01mif\u001b[39;00m pair[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39mtag]\n\u001b[1;32m      7\u001b[0m count_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tag_list)\n\u001b[0;32m----> 8\u001b[0m w_given_tag_list \u001b[38;5;241m=\u001b[39m [pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m tag_list \u001b[38;5;28;01mif\u001b[39;00m pair[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39mword]\n\u001b[1;32m      9\u001b[0m count_w_given_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(w_given_tag_list)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (count_w_given_tag, count_tag)\n","Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m tag_list \u001b[38;5;241m=\u001b[39m [pair \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m train_bag \u001b[38;5;28;01mif\u001b[39;00m pair[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39mtag]\n\u001b[1;32m      7\u001b[0m count_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tag_list)\n\u001b[0;32m----> 8\u001b[0m w_given_tag_list \u001b[38;5;241m=\u001b[39m [pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m tag_list \u001b[38;5;28;01mif\u001b[39;00m pair[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39mword]\n\u001b[1;32m      9\u001b[0m count_w_given_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(w_given_tag_list)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (count_w_given_tag, count_tag)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#Code to test all the test sentences\n","#(takes alot of time to run)\n","# tagging the test sentences()\n","# YOUR CODE HERE\n","\n","test_untagged_words = [tup[0] for sent in test_set for tup in sent]\n"," \n","start = time.time()\n","tagged_seq = Viterbi(test_untagged_words)\n","end = time.time()\n","difference = end-start\n"," \n","print(\"Time taken in seconds: \", difference)\n"," \n","# accuracy\n","check = [i for i, j in zip(test_tagged_words, test_untagged_words) if i == j] \n"," \n","accuracy = len(check)/len(tagged_seq)\n","print('Viterbi Algorithm Accuracy: ',accuracy*100)"]},{"cell_type":"markdown","metadata":{"id":"yjRW-lzcjcHn"},"source":["Tăng độ chính xác bằng cách kết hợp thêm rule"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Ku1eb3dtjcHo"},"outputs":[],"source":["#To improve the performance,we specify a rule base tagger for unknown words \n","# specify patterns for tagging\n","patterns = [\n","    (r'.*ing$', 'VERB'),              # gerund\n","    (r'.*ed$', 'VERB'),               # past tense \n","    (r'.*es$', 'VERB'),               # verb    \n","    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n","    (r'.*s$', 'NOUN'),                # plural nouns\n","    (r'\\*T?\\*?-[0-9]+$', 'X'),        # X\n","    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n","    (r'.*', 'NOUN')                   # nouns\n","]\n"," \n","# rule based tagger\n","rule_based_tagger = nltk.RegexpTagger(patterns)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"MpRBoJ9LjcHo"},"outputs":[],"source":["#modified Viterbi to include rule based tagger in it\n","def Viterbi_rule_based(words, train_bag = train_tagged_words):\n","    state = []\n","    T = list(set([pair[1] for pair in train_bag]))\n","     \n","    for key, word in enumerate(words):\n","        #initialise list of probability column for a given observation\n","        p = [] \n","        for tag in T:\n","            if key == 0:\n","                transition_p = tags_df.loc['.', tag]\n","            else:\n","                transition_p = tags_df.loc[state[-1], tag]\n","                 \n","            # compute emission and state probabilities\n","            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n","            state_probability = emission_p * transition_p    \n","            p.append(state_probability)\n","             \n","        pmax = max(p)\n","        state_max = rule_based_tagger.tag([word])[0][1]       \n","        \n","         \n","        if(pmax==0):\n","            state_max = rule_based_tagger.tag([word])[0][1] # assign based on rule based tagger\n","        else:\n","            if state_max != 'X':\n","                # getting state for which probability is maximum\n","                state_max = T[p.index(pmax)]                \n","             \n","         \n","        state.append(state_max)\n","    return list(zip(words, state))"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"XoiDF8Y5jcHp"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken in seconds:  22.01218318939209\n","Viterbi Algorithm Accuracy:  95.31772575250837\n"]}],"source":["#test accuracy on subset of test data \n","start = time.time()\n","tagged_seq = Viterbi_rule_based(test_tagged_words)\n","end = time.time()\n","difference = end-start\n"," \n","print(\"Time taken in seconds: \", difference)\n"," \n","# accuracy\n","check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n"," \n","accuracy = len(check)/len(tagged_seq)\n","print('Viterbi Algorithm Accuracy: ',accuracy*100)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"KeUQygYGjcHp"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('Will', 'NOUN'), ('can', 'VERB'), ('see', 'VERB'), ('Marry', 'NOUN')]\n","[('Will', 'CONJ'), ('can', 'VERB'), ('see', 'VERB'), ('Marry', 'CONJ')]\n"]}],"source":["#Check how a sentence is tagged by the two POS taggers\n","#and compare them\n","test_sent=\"Will can see Marry\"\n","pred_tags_rule=Viterbi_rule_based(test_sent.split())\n","pred_tags_withoutRules= Viterbi(test_sent.split())\n","print(pred_tags_rule)\n","print(pred_tags_withoutRules)\n","#Will and Marry are tagged as NUM as they are unknown words for Viterbi Algorithm "]}],"metadata":{"colab":{"name":"POS_tagging_HMM.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
