{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6Iu9BoERWaD"
      },
      "source": [
        "### **Bài 12: Bài tập về nhà: Ứng dụng mô hình sequence2sequence cho bài toán sinh văn bản (text generation)**\n",
        "Tổng quan: Ở bài tập này chúng ta sẽ ôn lại cách xây dựng và sử dụng mô hình seq2seq cho bài toán sinh văn bản."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiPDIh18dWVt"
      },
      "source": [
        "**1. Chuẩn bị dữ liệu và tiền xử lý**\n",
        "\n",
        "Trong bài tập này chúng ta sẽ xử lý dữ liệu của bài toán tóm tắt văn bản để thực nghiệm cho bài toán sinh văn bản. Trong bài toán tóm tắt văn bản, input của chương trình sẽ là 1 văn bản dài và output sẽ là 1 văn bản ngắn hơn và chứa những thông tin quan trọng của văn bản đầu vào. Ngược lại với bài toán trên, trong bài toán sinh văn bản, chúng ta muốn input đầu vào là 1 vài keyword hoặc 1 đoạn văn ngắn và output ra 1 đoạn văn dài. Vì thế, ta hoàn toàn có thể sử dụng dữ liệu trong bài toán tóm tắt văn bản để huấn luyện cho bài toán sinh văn bản, với input là câu đã được tóm tắt và output là đoạn văn gốc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vUbu5jijD8T"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9mVYgn2fVy5"
      },
      "source": [
        "Bài tập 1: Bạn hãy download 1 bộ dữ liệu tóm tắt văn bản và tiền xử lý chúng. Có thể dùng 1 trong các bộ dữ liệu dưới đây\n",
        "*   Bộ gigaword : https://drive.google.com/open?id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\n",
        "*   Bộ CNN/DM : https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz\n",
        "\n",
        "Yêu cầu : Sau khi tiền xử lý, chúng ta sẽ có 4 file data gồm :\n",
        "*   train.input.txt : Chứa các câu tóm tắt dùng để huấn luyện mô hình, thường chiếm 80% kích thước tổng dữ liệu\n",
        "*   train.output.txt : Chứa các đoạn văn bản gốc ứng với các tóm tắt.\n",
        "*   valid.input.txt : Chứa các câu tóm tắt dùng để đánh giá mô hình, thường chiếm 10% kích thưởng tổng dữ liệu.\n",
        "*   valid.output.txt : Chứa các đoạn văn bản gốc ứng vs các tóm tắt.\n",
        "\n",
        "Lưu ý : Nếu bạn để max_length của dữ liệu quá lớn, thì mô hình của bạn sẽ rất to và có thể gây tràn RAM.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz"
      ],
      "metadata": {
        "id": "T5rLbUSbD57y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d65935f4-8f83-4cc6-a505-76baa3fe8854"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-18 04:11:46--  https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.134.96, 16.182.73.112, 54.231.227.72, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.134.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500375629 (477M) [application/x-gzip]\n",
            "Saving to: ‘cnndm.tar.gz’\n",
            "\n",
            "cnndm.tar.gz        100%[===================>] 477.20M  52.9MB/s    in 9.5s    \n",
            "\n",
            "2024-10-18 04:11:56 (50.2 MB/s) - ‘cnndm.tar.gz’ saved [500375629/500375629]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf cnndm.tar.gz"
      ],
      "metadata": {
        "id": "hySVmj9fES1I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2113f35b-1ad6-4a0f-e20c-bcbfd6ebd2d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.txt.src\n",
            "test.txt.tgt.tagged\n",
            "train.txt.src\n",
            "train.txt.tgt.tagged\n",
            "val.txt.src\n",
            "val.txt.tgt.tagged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai4Rg9N8fL7q"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = \"<start> \"+sentence.strip()+\" <end>\"\n",
        "    return sentence\n",
        "\n",
        "def load_data(source_file, target_file, number_of_examples):\n",
        "    max_len = 50\n",
        "    source_sents = open(source_file, \"r\").readlines()\n",
        "    target_sents = open(target_file, \"r\").readlines()\n",
        "    assert len(source_sents) == len(target_sents)\n",
        "\n",
        "    source_data, target_data = [], []\n",
        "    for source_sentence, target_sentence in zip(source_sents[:number_of_examples],\n",
        "                                                target_sents[:number_of_examples]):\n",
        "        if len(source_sentence.strip().split()) > max_len or len(target_sentence.strip().split()) > max_len:\n",
        "            continue\n",
        "        source_data.append(preprocess_sentence(source_sentence))\n",
        "        target_data.append(preprocess_sentence(target_sentence))\n",
        "\n",
        "    return source_data, target_data\n",
        "\n",
        "def tokenizer(sentences):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters='')\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    sent_tensors = tokenizer.texts_to_sequences(sentences)\n",
        "    sent_tensors = tf.keras.preprocessing.sequence.pad_sequences(sent_tensors,\n",
        "                                                            padding='post')\n",
        "\n",
        "    return sent_tensors, tokenizer\n",
        "\n",
        "def create_data(source_path, target_path, number_of_examples):\n",
        "  # YOUR CODE HERE\n",
        "    source_data, target_data = load_data(source_path, target_path, number_of_examples)\n",
        "    source_tensors, source_tokenizer = tokenizer(source_data)\n",
        "    target_tensors, target_tokenizer = tokenizer(target_data)\n",
        "    return source_tensors, target_tensors, source_tokenizer, target_tokenizer\n",
        "\n",
        "number_of_examples = -1\n",
        "train_src_tensors, train_tgt_tensors, train_src_tokenizer, train_tgt_tokenizer = create_data(\"train.txt.src\", \"train.txt.tgt.tagged\", number_of_examples)\n",
        "# valid_src_tensors, valid_tgt_tensors, _, _ = create_data(\"val.txt.src\", \"val.txt.tgt.tagged\", -1)\n",
        "\n",
        "max_length_source, max_length_target = train_src_tensors.shape[1], train_tgt_tensors.shape[1]\n",
        "# print(len(train_src_tensors), len(valid_src_tensors))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jntYQNQDhQJy"
      },
      "source": [
        "**2. Sử dụng tf.data.Dataset để tạo dữ liệu huấn luyện**\n",
        "\n",
        "Bài tập 2: Hãy xem lại bài tập seq2seq cho bài toán dịch máy để thực hiện cách build data theo batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh86ntmfdQoF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c379f956-c32b-4515-b223-e971dee9131d"
      },
      "source": [
        "BUFFER_SIZE = len(train_src_tensors)\n",
        "BATCH_SIZE = 64\n",
        "vocab_src_size = len(train_src_tokenizer.word_index)+1\n",
        "vocab_tgt_size = len(train_tgt_tokenizer.word_index)+1\n",
        "steps_per_epoch = len(train_src_tensors)//BATCH_SIZE\n",
        "# YOUR CODE HERE\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_src_tensors, train_tgt_tensors))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 52]), TensorShape([64, 52]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boAbxfdYh25B"
      },
      "source": [
        "**3. Mô hình Seq2Seq với Attention**\n",
        "\n",
        "Bài tập 3: Hãy viết lại các thành phần Encoder, Attention, Decoder theo cách hiểu của bạn. Ngoài BahdanauAttention, LuongAttention cũng rất phổ biến, bạn hãy thử implement LuongAttention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6EdqDh2iXg4"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n",
        "        # YOUR CODE HERE\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_state_size = hidden_state_size\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.gru = tf.keras.layers.GRU(self.hidden_state_size,\n",
        "                                        return_sequences=True,\n",
        "                                        return_state=True,)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        # YOUR CODE HERE\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        # YOUR CODE HERE\n",
        "        return tf.zeros((self.batch_sz, self.hidden_state_size))\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_state_size = hidden_state_size\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.gru = self.gru = tf.keras.layers.GRU(self.hidden_state_size,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True,)\n",
        "\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.hidden_state_size)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 512\n",
        "hidden_state_size = 512\n",
        "\n",
        "encoder = Encoder(vocab_src_size, embedding_dim, hidden_state_size, BATCH_SIZE)\n",
        "attention_layer = BahdanauAttention(10)\n",
        "decoder = Decoder(vocab_tgt_size, embedding_dim, hidden_state_size, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "vp3HsQHWJbYI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rWDlRvCiuk5"
      },
      "source": [
        "**4. Hàm tối ưu và hàm lỗi**\n",
        "\n",
        "Hàm tối ưu Adam và hàm lỗi Cross Entropy được dùng rất phổ biến trong các mô hình học sâu, trong phần này, chúng ra sẽ dùng lại các hàm đó nhé."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYs4tec9i_k6"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(real != 0, dtype=tf.float32)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UOgKpkcjJXo"
      },
      "source": [
        "**5. Huấn luyện**\n",
        "\n",
        "Giờ đã có đủ nguyên liệu và mô hình rồi, ta hãy cùng huấn luyện 1 mô hình có thể sinh văn bản."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuZoHVLqjr70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "326e471c-af2f-4341-d985-10e8dafd26b4"
      },
      "source": [
        "@tf.function\n",
        "def train_step(source, target, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(source, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([train_tgt_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for t in range(1, target.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(target[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(target[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(target.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss\n",
        "\n",
        "\n",
        "checkpoint_dir = './model_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "EPOCHS = 1\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    for (batch, (source, target)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(source, target, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                    batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 1 epochs\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 5.5485\n",
            "Epoch 1 Loss 5.5485\n",
            "Time taken for 1 epoch 113.86390805244446 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSClgbvGjMOR"
      },
      "source": [
        "**6. Sinh văn bản**\n",
        "\n",
        "Hãy sử dụng mô hình vừa được huấn luyện để thực hiện sinh văn bản."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(input_sentence):\n",
        "  # YOUR CODE HERE\n",
        "    attention_plot = np.zeros((max_length_target, max_length_source))\n",
        "    source_sentence = preprocess_sentence(input_sentence)\n",
        "\n",
        "    # Handle out-of-vocabulary (OOV) words by mapping unknown words to a special token\n",
        "    inputs = []\n",
        "    for word in source_sentence.lower().split(' '):\n",
        "        if word in train_src_tokenizer.word_index:\n",
        "            inputs.append(train_src_tokenizer.word_index[word])\n",
        "        else:\n",
        "            # Use the OOV token index if the word is not found in the vocabulary\n",
        "            inputs.append(train_src_tokenizer.word_index.get('<oov>', 0))  # Assuming '<oov>' is your OOV token\n",
        "\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_source,\n",
        "                                                         padding='post')\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "  # YOUR CODE HERE\n",
        "    # Initialize hidden state\n",
        "    hidden = encoder.initialize_hidden_state()[:inputs.shape[0]]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([train_tgt_tokenizer.word_index['<start>']] * inputs.shape[0], 1)\n",
        "\n",
        "    for t in range(max_length_target):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.squeeze(attention_weights)\n",
        "        attention_plot[t] = attention_weights\n",
        "\n",
        "        predicted_id = np.argmax(predictions)\n",
        "\n",
        "        # Ensure the predicted_id exists in the tokenizer's index_word\n",
        "        if predicted_id in train_tgt_tokenizer.index_word:\n",
        "            result += ' ' + train_tgt_tokenizer.index_word[predicted_id]\n",
        "        else:\n",
        "            result += ' <unknown>'  # Handle cases where the predicted ID is not found\n",
        "\n",
        "        # Pass the predicted_id to the decoder for the next time step\n",
        "        dec_input = tf.expand_dims([predicted_id], 1)\n",
        "\n",
        "    return result, source_sentence, attention_plot\n",
        "\n",
        "\n",
        "\n",
        "input_sentence = \"hollywood shores up support for ocean 's thirteen\"\n",
        "original_article = \"hollywood is planning a new sequel to adventure flick `` ocean 's eleven , '' with star george clooney set to reprise his role as a charismatic thief in `` ocean 's thirteen , '' the entertainment press said wednesday .\"\n",
        "result, input_sentence, attention_plot = evaluate(input_sentence)\n",
        "print('Input: %s' % (input_sentence))\n",
        "print('Output : {}'.format(result))"
      ],
      "metadata": {
        "id": "zMM5GRgONevM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3a8821d5-be19-4b90-b11a-d4312e0e529f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> hollywood shores up support for ocean 's thirteen <end>\n",
            "Output :  <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t> . </t> <t>\n"
          ]
        }
      ]
    }
  ]
}